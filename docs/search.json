[
  {
    "objectID": "webfix.html#groundhog-day-predictions",
    "href": "webfix.html#groundhog-day-predictions",
    "title": "DS002R Projects Presentation",
    "section": "Groundhog Day Predictions",
    "text": "Groundhog Day Predictions\n\nData on New York Times headlines from RTextTools package\n\nDate, title, subject of headlines from 1996-2006\n\nCompared total NYT headlines mentions between Bill Clinton and George W. Bush during their consecutive terms (Clinton 1997-2000 and Bush 2001-2004)\nExamined word length of headlines across months, with and without punctuation"
  },
  {
    "objectID": "webfix.html#the-simpsonss-script",
    "href": "webfix.html#the-simpsonss-script",
    "title": "DS002R Projects Presentation",
    "section": "The Simpsons’s Script",
    "text": "The Simpsons’s Script\n\n\nLooking at characters with over 2000 cumulative word counts.\nHow is line distribution decided based on a character’s role?"
  },
  {
    "objectID": "webfix.html#character-word-count",
    "href": "webfix.html#character-word-count",
    "title": "DS002R Projects Presentation",
    "section": "Character word count",
    "text": "Character word count"
  },
  {
    "objectID": "webfix.html#permutation",
    "href": "webfix.html#permutation",
    "title": "DS002R Projects Presentation",
    "section": "Permutation’",
    "text": "Permutation’"
  },
  {
    "objectID": "webfix.html#p-value",
    "href": "webfix.html#p-value",
    "title": "DS002R Projects Presentation",
    "section": "P-value",
    "text": "P-value"
  },
  {
    "objectID": "webfix.html#florida-vs-california",
    "href": "webfix.html#florida-vs-california",
    "title": "DS002R Projects Presentation",
    "section": "Florida vs California",
    "text": "Florida vs California\n\n."
  },
  {
    "objectID": "webfix.html#proportion",
    "href": "webfix.html#proportion",
    "title": "DS002R Projects Presentation",
    "section": "Proportion",
    "text": "Proportion\n\n\n [1] \"ar_little_rock_2020_04_01\"      \"az_gilbert_2020_04_01\"         \n [3] \"az_mesa_2023_01_26\"             \"az_statewide_2020_04_01\"       \n [5] \"ca_anaheim_2020_04_01\"          \"ca_bakersfield_2020_04_01\"     \n [7] \"ca_long_beach_2020_04_01\"       \"ca_los_angeles_2020_04_01\"     \n [9] \"ca_oakland_2020_04_01\"          \"ca_san_bernardino_2020_04_01\"  \n[11] \"ca_san_diego_2020_04_01\"        \"ca_san_francisco_2020_04_01\"   \n[13] \"ca_san_jose_2020_04_01\"         \"ca_santa_ana_2020_04_01\"       \n[15] \"ca_statewide_2023_01_26\"        \"ca_stockton_2020_04_01\"        \n[17] \"co_aurora_2023_01_26\"           \"co_denver_2020_04_01\"          \n[19] \"co_statewide_2020_04_01\"        \"ct_hartford_2020_04_01\"        \n[21] \"ct_statewide_2020_04_01\"        \"fl_saint_petersburg_2020_04_01\"\n[23] \"fl_statewide_2020_04_01\"        \"fl_tampa_2020_04_01\"           \n[25] \"ga_statewide_2020_04_01\"        \"ia_statewide_2020_04_01\"       \n[27] \"id_idaho_falls_2020_04_01\"      \"il_chicago_2023_01_26\"         \n[29] \"il_statewide_2020_04_01\"        \"in_fort_wayne_2020_04_01\"      \n[31] \"ks_wichita_2023_01_26\"          \"ky_louisville_2023_01_26\"      \n[33] \"ky_owensboro_2020_04_01\"        \"la_new_orleans_2020_04_01\"     \n[35] \"ma_statewide_2020_04_01\"        \"md_baltimore_2020_04_01\"       \n[37] \"md_statewide_2020_04_01\"        \"mi_statewide_2020_04_01\"       \n[39] \"mn_saint_paul_2020_04_01\"       \"mo_statewide_2020_04_01\"       \n[41] \"ms_statewide_2020_04_01\"        \"mt_statewide_2023_01_26\"       \n[43] \"nc_charlotte_2020_04_01\"        \"nc_durham_2020_04_01\"          \n[45] \"nc_fayetteville_2020_04_01\"     \"nc_greensboro_2020_04_01\"      \n[47] \"nc_raleigh_2020_04_01\"          \"nc_statewide_2020_04_01\"       \n[49] \"nc_winston_salem_2020_04_01\"    \"nd_grand_forks_2020_04_01\"     \n[51] \"nd_statewide_2020_04_01\"        \"ne_statewide_2020_04_01\"       \n[53] \"nh_statewide_2020_04_01\"        \"nj_camden_2020_04_01\"          \n[55] \"nj_statewide_2020_04_01\"        \"nv_henderson_2020_04_01\"       \n[57] \"nv_statewide_2020_04_01\"        \"ny_albany_2020_04_01\"          \n[59] \"ny_statewide_2020_04_01\"        \"oh_cincinnati_2020_04_01\"      \n[61] \"oh_columbus_2020_04_01\"         \"oh_statewide_2020_04_01\"       \n[63] \"ok_oklahoma_city_2023_01_26\"    \"ok_tulsa_2020_04_01\"           \n[65] \"or_statewide_2020_04_01\"        \"pa_philadelphia_2020_04_01\"    \n[67] \"ri_statewide_2020_04_01\"        \"sc_statewide_2020_04_01\"       \n[69] \"sd_statewide_2020_04_01\"        \"tn_nashville_2020_04_01\"       \n[71] \"tn_statewide_2020_04_01\"        \"tx_arlington_2020_04_01\"       \n[73] \"tx_austin_2020_04_01\"           \"tx_garland_2020_04_01\"         \n[75] \"tx_houston_2023_01_26\"          \"tx_lubbock_2020_04_01\"         \n[77] \"tx_plano_2020_04_01\"            \"tx_san_antonio_2023_01_26\"     \n[79] \"tx_statewide_2020_04_01\"        \"va_statewide_2020_04_01\"       \n[81] \"vt_burlington_2023_01_26\"       \"vt_statewide_2020_04_01\"       \n[83] \"wa_seattle_2020_04_01\"          \"wa_statewide_2020_04_01\"       \n[85] \"wa_tacoma_2020_04_01\"           \"wi_madison_2023_01_26\"         \n[87] \"wi_statewide_2020_04_01\"        \"wy_statewide_2020_04_01\""
  },
  {
    "objectID": "Riskprofile.html",
    "href": "Riskprofile.html",
    "title": "English Education in Small Towns",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 4)\n\n---- Compiling #TidyTuesday Information for 2024-01-23 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"english_education.csv\"\n\nenglish_education &lt;- tuesdata$english_education\n\nIn this analysis, I will utilize the data from TidyTuesday 2024-01-23 to perform a permutation test. The goal of the test is to confirm the claims of the article in which the data was used to answer the question: Is there a significant difference in education scores between different-sized towns? The data is taken from the UK Office for National Statistics.\nLink to the data:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-01-23/readme.md\nhttps://www.ons.gov.uk/\n\nsize_interest &lt;- english_education |&gt;\n  filter(size_flag == \"Medium Towns\"| size_flag ==\"Small Towns\", na.rm = TRUE)\n\nThe selected variable\nSingle permutation\n\npermutation_test &lt;- function(reps,data) {\n  data |&gt;\n    select(size_flag, education_score)|&gt;\n    mutate(edu_perm = sample(education_score, replace = FALSE)) |&gt;\n    group_by(size_flag) |&gt;\n    summarize(mean_score = mean(education_score, na.rm = TRUE), mean_perm = mean(edu_perm)) |&gt;\n    summarize(mean_diff = abs(diff(mean_score)), mean_perm_diff = diff(mean_perm))\n  }\n\nRepeated simulated trials\n\nset.seed(123)\nunder_null &lt;- map(1:10000, permutation_test, data = size_interest) |&gt;\n  list_rbind()\n\nPlot distribution\n\nggplot(under_null, aes(x = mean_perm_diff)) +\n  geom_histogram(binwidth = 0.1, color = \"black\", alpha = 0.7) +\n  geom_vline(aes(xintercept = mean_diff, color = \"red\")) +\n  labs(title = \"Distribution of Permutation Under Null\",\n       x = \"Difference in Mean Scores\",\n       y = \"Frequency\") \n\n\n\n\n\n\n\n\nFrom the plot it can be seen that the line representing the observational measurement of education score differences is off-centered. There is a small number of occurrences of simulated under the null. This would mean that the observed difference is more extreme than data created randomly.\n\nunder_null|&gt;\nsummarize(p_value = mean(mean_perm_diff &gt; mean_diff))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0139\n\n\nAfter creating 10000 permutation simulations, the distribution under the null hypothesis falls on zero as there is no expected difference between the two groups. From the results of the p-value, if the significance level, as it usually is, is set to be 0.05, our p-value fall below this level. \\(\\alpha\\) &gt; 0.0139. This means that our data is significantly extreme and would probably not occur by chance."
  },
  {
    "objectID": "FedPapers.html",
    "href": "FedPapers.html",
    "title": "Federalist Papers Analysis",
    "section": "",
    "text": "Introduction\nThis is project 2, which seeks to work with string and character data. The data used comes from nicholasjhorton’s GitHub repository, which provides a data frame for the Federalist Papers. The text files are taken from Project Gutenberg. The data can be found here: https://github.com/nicholasjhorton/FederalistPapers/ .\nEstablishing a Government\nSince the purpose of the Federalist Papers is to defend the Constitution that establishes the U.S government branches, I proceeded to examine how many times this purpose is linked through mentions of the words “government”. I provided an alternative “confederacy” due to its predecessor, the Articles of Confederation, as a name for government in reference to the United States.\n\nCountry_FedPapers &lt;- FederalistPapers |&gt;\n  mutate(Country_FedPapers = str_to_lower(text))|&gt;\n  group_by(paper) |&gt;\n  summarise(Word_count = sum(str_count(text,\"government\") + \n              str_count(text,\"confederacy\"))) \nhead(Country_FedPapers)\n\n# A tibble: 6 × 2\n  paper Word_count\n  &lt;dbl&gt;      &lt;int&gt;\n1     1          6\n2     2          9\n3     3         18\n4     4         22\n5     5          6\n6     6          3\n\n\nAlthough counting the occurrence of the two words does not give any context to what direction the author intends to take the paper, the repetition of the words provides a hint.\nGraphing Count of “Government” and “Confederacy”\n\nggplot(Country_FedPapers, aes(x = paper, y = Word_count)) +\n  geom_col() + \n  labs(\n    x = \"Federalist Paper Number\",\n    y = \"Word Count\", \n    title = \"Number of times 'confederacy' or 'government' mentioned\"\n  )\n\n\n\n\n\n\n\n\nFrom the graph, we can see a consistent call upon government and confederacy with the highest count being paper 46 which describes the power of the government relative to state governments, so it is possible the constant mention of the two forms of government caused the spike.\nTitle Length per Paper\nAnother alteration to the data I will apply is separating the title into its generic FEDERALIST No. and its assigned title. From here I analyze the length of each title.\n\nNeater_Fedpaper &lt;- FederalistPapers |&gt;\n  mutate(solo_title = str_extract(title,\"(?&lt;=\\\\. ).+\")) |&gt;\n  group_by(paper) |&gt;\n  summarize(title_length = str_length(first(solo_title)))\n\nGraph of Title Length vs Specific Word Count\nThen will the length of the title tell anything about the wordiness of the author? In other words, will the length of the title predict the amount of times the words confederacy and government appear?\n\nJoined_Fedpaper &lt;- left_join(Country_FedPapers, Neater_Fedpaper, by = \"paper\")\n\nggplot(Joined_Fedpaper, aes( x = title_length , y = Word_count)) +\n  geom_point() +\n  geom_smooth() +\n  labs(\n    x = \"Length of Title\",\n    y = \"Count of government and confederacy\",\n    title = \"Does title length predict utterances of government?\"\n  )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is no easily visible marked trend. A longer title does not relate to a longer string of confederacies and government mentions that may appear in longer papers.\nConstitution and What Else?\nIn an attempt to contextualize what is referred to as the Constitution in the Federalist papers, the following code draws out the words following the word constitution.\n\nFederalistPapers |&gt;\n  filter(str_detect(text, \"(?i)constitution\"))|&gt;\n  group_by(paper)|&gt;\n  summarize(surr_const = str_c(unlist(\n    str_extract_all(text, \"(?i)constitution\\\\s+(\\\\w+)\")), collapse = \", \"))  \n\n# A tibble: 77 × 2\n   paper surr_const                                                            \n   &lt;dbl&gt; &lt;chr&gt;                                                                 \n 1     1 \"Constitution for, CONSTITUTION TO, CONSTITUTION and, Constitution or\"\n 2     6 \"\"                                                                    \n 3     7 \"\"                                                                    \n 4     8 \"constitution that\"                                                   \n 5     9 \"constitution that\"                                                   \n 6    10 \"Constitution forms\"                                                  \n 7    14 \"Constitution From, Constitution are, Constitution is\"                \n 8    15 \"constitution of\"                                                     \n 9    16 \"Constitution which, constitution in\"                                 \n10    17 \"\"                                                                    \n# ℹ 67 more rows\n\n\nAlthought the code does not give much context as the words following the mention of constitution are largely filler words, it does highlight the different instances of constitution. The capitalized Constitution is likely a direct reference to the U.S. Constitution. The completely capitalized CONSTITUTION is likely referenced in an address to the reader or some outstanding instance while the completely lowercase constitution is the general constitute of something.\nWho Wrote the Most?\nFrom a brief look at the authorship of each paper, it can be seen that Hamilton took authorship of most, but how many words did they write across their papers?\n\n FederalistPapers |&gt;\n  group_by(author) |&gt;\n  summarize(total_words = sum(str_count(text, \"\\\\S+\")))\n\n# A tibble: 3 × 2\n  author   total_words\n  &lt;chr&gt;          &lt;int&gt;\n1 Hamilton      114076\n2 Jay             8513\n3 Madison        69377\n\n\nUnsurprisingly, Hamilton wrote like he was running out of time and ended with 114,076 words across his many authored papers."
  },
  {
    "objectID": "dataviz2.html",
    "href": "dataviz2.html",
    "title": "Groundhog",
    "section": "",
    "text": "Introduction\nThis project draws its data from the GitHub repository TidyTuesday’s 2024-01-30 folder titled “Groundhog Predictions.” This data is originally from https://groundhog-day.com/. This data puts together the predictions of groundhogs for 6 more weeks of winter or the coming of spring across the US and Canada. That is to say, several groundhogs’ predictions are recorded every year.\n\nshadow_prediction  &lt;- predictions |&gt;\n  group_by(year) |&gt;\n  summarize(total_shadow_count = sum(shadow, na.rm = TRUE))\n\nggplot(shadow_prediction, aes(x = year, y = total_shadow_count)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nGraphing and Code\n\nid_prediction  &lt;- predictions |&gt;\n  group_by(id) |&gt;\n  summarize(total_shadow_count = sum(shadow, na.rm = TRUE))\n\nggplot(id_prediction,  aes(x = id, y = total_shadow_count)) +\n  geom_col() +\n  labs(\n    title = \"Longer winter predictions per Ground Hog\",\n    x = \"Groundhog identification\",\n    y = \"Saw Shadow Count\"\n  ) + theme_minimal()\n\n\n\n\n\n\n\nggplot(predictions, aes(x = year, fill = shadow)) +\n  geom_bar() +\n  labs(\n    title = \"Yearly Shadow Predictions\",\n    x = \"Year\",\n    y = \"Total Predictions\"\n  ) + theme_minimal()\n\n\n\n\n\n\n\n\nAnalysis\nThe first graph isolates the total count of observed shadows by year. Similarly, the third graph reiterates this idea by presenting the count of all shadow predictions per year. From these graphs, it is obvious that there is a growing number of predictions by different groundhogs. The growth in groundhog day popularity is especially visible when we view the third graph in conjunction with the second graph showing the consistency and recency in groundhog count of predictions. In the second graph, we can see that earlier groundhogs have had a greater impact on the data for winter predictions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Simpsons’s Script",
    "section": "",
    "text": "Introduction\nIn this project, I will use data from the TidyTuesday folder for February 2, 2025. The original data is from https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset, which is a kraggle data set authored by Prashant Banerjee. This data is further scraped by other users. The data is split into four data frames of characters, episodes, locations, and script lines. I focused on script line count among the characters with the over 2000 cumulative word counts.\nGraph and Code\n\nknown_character &lt;- simpsons_script_lines |&gt;\n  group_by(raw_character_text) |&gt;\n  summarize(total_word_count = sum(word_count, na.rm = TRUE)) |&gt;\n  filter(total_word_count &gt; 2000)|&gt;\n  arrange(total_word_count)\n\n\n  ggplot(known_character, aes(y = reorder(raw_character_text, -total_word_count), x = total_word_count)) +\n  geom_col() +\n  labs(\n    title = \"Characters with the Highest Word Count\",\n    y = \"Characters by Name\",\n    x = \"Total Word Count\"\n  ) + theme_minimal()\n\n\n\n\n\n\n\n\nAnalysis\nFor this data, we can see that the characters that have said over 2000 words are limited to 16 characters. Characters in the main cast naturally have more lines, but there is a large difference between the character with the highest word count (Homer) and the character with the second highest word count (Marge). This can be attributed to Homer being the protagonist. When we exclude the Simpsons family, Moe is the character with the most words said. There is a notable drop between Moe and Lisa Simpson, signaling the end of the main cast. Would an avid Simpsons fan consider this character’s word count surprising?"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Ethics in Predictive Analytics for Marketing",
    "section": "",
    "text": "Two articles:\n\nHow Target leveraged predictive analytics to improve marketing ROI. OvationMR. Retrieved from https://www.ovationmr.com/how-target-leveraged-predictive-analytics/\n\n\n\nDuhigg, C. (2012, February 16). How companies learn your secrets. The New York Times Magazine. https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\n\nSummary\nIn 2012, the New York Times released an article with the headline, “How Companies Learn Your Secrets”. The big story of this article was how Target asked a statistician, Andrew Pole, whom they hired to identify pregnant customers so they could tailor their marketing before any other company could capitalize on the spending habits of new parents. The article further dives into the work of predictive analytics and how companies use customer habits to determine how to better sell a product. The reporter, Charles Duhigg, states that when Target was made aware of his article, they stated that they were in compliance with Privacy Laws and critiqued inaccuracies in his reporting, but would not elaborate or meet with him for further discussion. Further development by Pole on the targeted advertisement was to address possibly scaring their customers by alerting them they knew their personal profile or information before they had a purchase history of it. To overcome this, a layer of mixed advertisement with targeted advertisement would hide this.\nOvationMR takes the prior article discussed as a launching point for their own advertisement. The website uses the New York Times article as a case study to learn strategies and marketing growth from. Calling its resolution a success, with Target being able to “maintain its customer relationships while respecting their privacy.”\n\nWhat was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?\nFrom the New York Times article, it is shown the consent structure is not fully seen by the customer as it is a silent agreement that purchase history will be retained by the company. In this case, like in many marketing departments, a user profile is created for the customers to track engagement across what the store offers, including coupons, surveys, refunds, purchases, etc. The customers were not aware that Target would transform the data collected on them into a prediction system of their potential pregnancy status. Informed consent of the full situation was not sought after since this fits within company privacy laws that allow it to maintain information for marketing purposes, but a concent is implied to give the company data when purchasing from them. Although the purpose could be deduced early on when advertisements specifically showed baby items alone.\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations?\nThe data collection process was based on identifying customer behavior patterns and technically on legal grounds and respecting privacy, as Target only made conclusions based on what customers would purchase. OvationMR encourages the practice of data tracking with existing models and third party resources that follow privacy laws. This further drives the point of widespread data tracking.\nIs the data identifiable? All of it? Some of it? In what way? Are the data sufficiently anonymized or old to be free of ethical concerns? Is anonymity guaranteed?\nWhile this data is not publicly available, in earlier advertising efforts, it could be deduced that pregnancy was implied and therefore gave a piece of private information to whoever encountered the advertisement in connection with the customer, as was the case for the angry father and his teenage daughter. The revised advertisement model better hides the privacy issue as it is theoretically undetectable even by the target customer. However, holding the information of someone being pregnant without their knowledge is a privacy issue on its own, as the customer has not sought to make this personal information available to Target. Forming such a sensible data set can risk customer privacy if there is a data breach. If third-party advertisement, as brought up by OvationMR, is taken into consideration, then information from Target can end up elsewhere.\n10.Respect and invite fair criticism while promoting the identification and open discussion of errors, risks, and unintended consequences of our work.\nOvationMR reviewed the criticism Target received as a need to reanalyze their marketing strategy rather than an overstep. From the New York Times, it is reported that Target refused to give a statement to Pole, which means in light of their predictive analytics, they failed a point of the manifesto. By refusing to engage in conversation over their data collection and analysis, they in some form acknowledged that it was not the most well-handled data. It refused direct criticism but answered to customer criticism not by removing the ads but by disguising them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Devi Canseco Lopez",
    "section": "",
    "text": "Devi is a rising Junior at Pomona College looking to graduate with a degree in Cognitive Science and a double minor in Latin American Studies and Data Science. She is a member of Pomona Farm Club, Pomona College Latinx Alliance, and Rooftop Garden. In her free time, she enjoys doing outdoor activities, drawing, and playing video games.\n\n\nPomona College | Claremont, CA\n\n\n\nR, Git, Python, Javascript, HTML/CSS, Excel"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Devi Canseco Lopez",
    "section": "",
    "text": "Pomona College | Claremont, CA"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Devi Canseco Lopez",
    "section": "",
    "text": "R, Git, Python, Javascript, HTML/CSS, Excel"
  },
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "SQL",
    "section": "",
    "text": "The data for this analysis comes from the Stanford Open Policing Project, published in Pierson et al. (2020). Which can be found in https://openpolicing.stanford.edu/data/. In this project, I want to compare reasonable driving stops for drivers across states. First, I will compare drivers from California and Florida with the tentative question: Which state has better drivers? To answer this question, I will find and compare the proportion of stops that resulted in a citation for each state. On a second analysis, I will utilize Philadelphia’s data frame to find that searches provided resulted in a reasonable cause when performed. This follows the question: Did they have the right suspicion? I will extract the number of searches that found contraband and compare it to the total number of searches conducted.\nBetter Drivers\nIn this analysis, the statewide reports of LA and FL are used to form a comparison of the proportion of drivers issued a citation.\n\nSELECT\n    state,\n    COUNT(*) AS total_stops,\n    SUM(citation_issued) AS citations,\n    ROUND(SUM(citation_issued) / COUNT(*), 2) AS citation_rate\nFROM (\n    SELECT\n        'CA' AS state,\n         citation_issued\n    FROM ca_statewide_2023_01_26\n\n    UNION ALL\n\n    SELECT\n        'FL' AS state,\n        citation_issued\n    FROM fl_statewide_2020_04_01\n) combined\nGROUP BY state\nORDER BY citation_rate DESC;\n\n\nhead(prop_table)\n\n  state total_stops citations citation_rate\n1    FL     7297538   3707393          0.51\n2    CA    31778515    739127          0.02\n\n\nThe table shows numerically how many stops and how many of those stops resulted in citations.\n\nggplot(prop_table, aes(x = state, y = citation_rate)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Proportion of Drivers Issued Citations by State Patrol\",\n    x = \"State\",\n    y = \"Citation Rate\"\n  ) \n\n\n\n\n\n\n\n\nFrom the graph, it can be seen that there is a large difference between the proportion of drivers who were stopped AND received a citation in California and Florida. While no conclusions can be drawn from this snippet of data, it can be speculated that perhaps Californian drivers are better drivers and did not commit violations that guaranteed a citation, or it could also propose the idea that traffic officers are more lenient to driving mistakes.\nEvidence Found\nIn this analysis, the data is taken from the Philadelphia, PA table. It seeks to find if searches resulted in finding evidence for a suspected holding of contraband.\n\nSELECT\n    COUNT(*) AS total_searches,\n    ROUND(SUM(CASE WHEN contraband_found = TRUE THEN 1 ELSE 0 END)) AS searches_with_contraband,\n    ROUND(SUM(CASE WHEN contraband_found = TRUE THEN 1 ELSE 0 END) / COUNT(*), 2) AS contraband_hit_rate \nFROM\n    pa_philadelphia_2020_04_01\nWHERE\n    search_conducted = TRUE\n    AND contraband_found IS NOT NULL;\n\n\ncontr_table\n\n  total_searches searches_with_contraband contraband_hit_rate\n1         116455                    33230                0.29\n\n\n\nsearch_table &lt;- contr_table |&gt;\n  mutate(searches_without_contraband = total_searches - searches_with_contraband)|&gt;\n  mutate(contraband_miss_rate = 100 - contraband_hit_rate)\n\n\nsearch_table &lt;- contr_table |&gt;\n  mutate(\n    searches_without_contraband = total_searches - searches_with_contraband\n  ) |&gt;\n  transmute(\n    `Contraband Found` = as.numeric(searches_with_contraband),\n    `No Contraband Found` = as.numeric(searches_without_contraband)\n  ) |&gt;\n  pivot_longer(\n    cols = everything(),\n    names_to = \"category\",\n    values_to = \"count\"\n  )\n\nggplot(search_table, aes(x = category, y = count, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Philadelphia Search Outcomes\",\n    y = \"Number of Searches\",\n    x = \"\"\n  )\n\n\n\n\n\n\n\n\n\nDBI::dbDisconnect(con_traffic)\n\nData Source: Stanford Open Policing Project. Pierson, E., Corbett-Davies, S., & Goel, S. (2020). “A large-scale analysis of racial disparities in police stops across the United States.” Nature Human Behaviour, 4(7), 736–745. https://openpolicing.stanford.edu/"
  }
]