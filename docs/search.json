[
  {
    "objectID": "dataviz2.html",
    "href": "dataviz2.html",
    "title": "Groundhog",
    "section": "",
    "text": "Introduction\nThis project draws its data from the GitHub repository TidyTuesday’s 2024-01-30 folder titled “Groundhog Predictions”. This data is originally from https://groundhog-day.com/. This data puts together the predictions of groundhogs for 6 more weeks of winter or the coming of spring across the US and Canada.\n\nshadow_prediction  &lt;- predictions |&gt;\n  group_by(year) |&gt;\n  summarize(total_shadow_count = sum(shadow, na.rm = TRUE))\nshadow_prediction\n\n# A tibble: 138 × 2\n    year total_shadow_count\n   &lt;dbl&gt;              &lt;int&gt;\n 1  1886                  0\n 2  1887                  1\n 3  1888                  1\n 4  1889                  0\n 5  1890                  0\n 6  1891                  0\n 7  1892                  0\n 8  1893                  0\n 9  1894                  0\n10  1895                  0\n# ℹ 128 more rows\n\nggplot(shadow_prediction, aes(x = year, y = total_shadow_count)) +\n  geom_col()\n\n\n\n\n\n\n\n\nGraphing and Code\n\nid_prediction  &lt;- predictions |&gt;\n  group_by(id) |&gt;\n  summarize(total_shadow_count = sum(shadow, na.rm = TRUE))\n\nggplot(id_prediction,  aes(x = id, y = total_shadow_count)) +\n  geom_col() +\n  labs(\n    title = \"Longer winter predictions per Ground Hog\",\n    x = \"Groundhog identification\",\n    y = \"Saw Shadow Count\"\n  )\n\n\n\n\n\n\n\nggplot(predictions, aes(x = year, fill = shadow)) +\n  geom_bar() +\n  labs(\n    title = \"Yearly Shadow Predictions\",\n    x = \"Year\",\n    y = \"Total Predictions\"\n  )\n\n\n\n\n\n\n\n\nAnalysis\nThe first graph isolates the total count of observed shadows by year. Similarly, the third graph reiterates this idea by presenting the count of all shadow predictions per year. From these graphs, it is obvious that there is a growing number of predictions by different groundhogs. The growth in groundhog day popularity is especially visible when we view the third graph in conjunction with the second graph showing the consistency and recency in groundhog count of predictions. In the second graph, we can see that earlier groundhogs have had a greater impact on the data for winter predictions."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Simpsons",
    "section": "",
    "text": "Introduction\nIn this project, I will use data from the TidyTuesday folder for February 2, 2025. The original data is from https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset, which is a kraggle data set authored by Prashant Banerjee. This data is further scraped by other users. The data is split into four data frames of characters, episodes, locations, and script lines. I focused on script line count among the characters with the over 2000 cumulative word counts.\nGraph and Code\n\nknown_character &lt;- simpsons_script_lines |&gt;\n  group_by(character_id) |&gt;\n  summarize(total_word_count = sum(word_count, na.rm = TRUE)) |&gt;\n  filter(total_word_count &gt; 2000)\nknown_character\n\n# A tibble: 16 × 2\n   character_id total_word_count\n          &lt;dbl&gt;            &lt;dbl&gt;\n 1            1            23903\n 2            2            52649\n 3            3             5023\n 4            8            22528\n 5            9            21675\n 6           11             4319\n 7           15             6841\n 8           17             8773\n 9           25             4075\n10           31             4815\n11           71             3962\n12          101             2067\n13          139             3857\n14          165             2944\n15          211             2271\n16         1078             2533\n\nknown_character|&gt;\n  ggplot(aes(x = character_id, y = total_word_count)) +\n  geom_point() +\n  labs(\n    title = \"Characters with the Highest Word Count\",\n    x = \"Character Identification\",\n    y = \"Total Word Count\"\n  )\n\n\n\n\n\n\n\n\nAnalysis\nFor this data, we can see that the characters that have said over 2000 words is limited to 16 characters. Characters that appear earlier in the character id line seem to be prominent to appear as prolific characters. The next step in this data is to identify a match between character id to character name across data frames to better understand which characters are favored for lines. If we were to exclude the main Simpson family, which characters would receive the most attention?"
  },
  {
    "objectID": "FedPapers.html",
    "href": "FedPapers.html",
    "title": "Federalist Papers Analysis",
    "section": "",
    "text": "Introduction\nThis is project 2 which seeks to work with string and character data. The data used comes from nicholasjhorton’s GitHub repository which provides a data frame for the Federalist papers.\nEstablishing a Government\nSince the purpose of the Federalist Papers is to defend the Constitution that establishes the U.S government branches, I proceeded to examine how many times this purpose is linked through mentions of the words “government”. I provided an alternative “confederacy” due to its predecessor, the Articles of Confederation, as a name for government in reference to the United States.\n\nCountry_FedPapers &lt;- FederalistPapers |&gt;\n  mutate(Country_FedPapers = str_to_lower(text))|&gt;\n  group_by(paper) |&gt;\n  summarise(Word_count = sum(str_count(text,\"government\") + \n              str_count(text,\"confederacy\"))) \n\nAlthough counting the occurrence of the two words does not give any context to what direction the author intends to take the paper, the repetition of the words provides a hint.\nGraphing Count of “Government” and “Confederacy”\n\nggplot(Country_FedPapers, aes(x = paper, y = Word_count)) +\n  geom_col() + \n  labs(\n    x = \"Federalist Paper Number\",\n    y = \"Word Count\", \n    title = \"Number of times 'confederacy' or 'government' mentioned\"\n  )\n\n\n\n\n\n\n\n\nFrom the graph, we can see a consistent call upon government and confederacy with the highest count being around paper 45 which describes the power of the government relative to state governments, so it is possible the constant mention of the two forms of government caused the spike.\nTitle Length per Paper\nAnother alteration to the data I will apply is separating the title into its generic FEDERALIST No. and its assigned title. From here I analyze the length of each title.\n\nNeater_Fedpaper &lt;- FederalistPapers |&gt;\n  mutate(solo_title = str_extract(title,\"(?&lt;=\\\\. ).+\")) |&gt;\n  group_by(paper) |&gt;\n  summarize(title_length = str_length(first(solo_title)))\n\nGraph of Title Length vs Specific Word Count\nThen will the length of the title tell anything about the wordiness of the author? In other words, will the length of the title predict the amount of times the words confederacy and government appear?\n\nJoined_Fedpaper &lt;- left_join(Country_FedPapers, Neater_Fedpaper)\n\nJoining with `by = join_by(paper)`\n\nggplot(Joined_Fedpaper, aes( x = title_length , y = Word_count)) +\n  geom_point() +\n  labs(\n    x = \"Length of Title\",\n    y = \"Count of government and confederacy\",\n    title = \"Does title length predict utterances of government?\"\n  )\n\n\n\n\n\n\n\n\nThere is no easily visible marked trend. A longer title does not relate to a longer string of confederacies and government mentions that may appear in longer papers.\nConstitution and What Else?\nIn an attempt to contextualize what is referred to as the Constitution in the Federalist papers, the following code draws out the words following the word constitution.\n\nFederalistPapers |&gt;\n  filter(str_detect(text, \"(?i)constitution\"))|&gt;\n  group_by(paper)|&gt;\n  summarize(surr_const = str_c(unlist(\n    str_extract_all(text, \"(?i)constitution\\\\s+(\\\\w+)\")), collapse = \", \"))  \n\n# A tibble: 77 × 2\n   paper surr_const                                                            \n   &lt;dbl&gt; &lt;chr&gt;                                                                 \n 1     1 \"Constitution for, CONSTITUTION TO, CONSTITUTION and, Constitution or\"\n 2     6 \"\"                                                                    \n 3     7 \"\"                                                                    \n 4     8 \"constitution that\"                                                   \n 5     9 \"constitution that\"                                                   \n 6    10 \"Constitution forms\"                                                  \n 7    14 \"Constitution From, Constitution are, Constitution is\"                \n 8    15 \"constitution of\"                                                     \n 9    16 \"Constitution which, constitution in\"                                 \n10    17 \"\"                                                                    \n# ℹ 67 more rows\n\n\nAlthought the code does not give much context as the words following the mention of constitution are largely filler words, it does highlight the different instances of constitution. The capitalized Constitution is likely a direct reference to the U.S. Constitution. The completely capitalized CONSTITUTION is likely referenced in an address to the reader or some outstanding instance while the completely lowercase constitution is the general constitute of something.\nWho Wrote the Most?\nFrom a brief look at the authorship of each paper, it can be seen that Hamilton took authorship of most, but how many words did they write across their papers?\n\n FederalistPapers |&gt;\n  group_by(author) |&gt;\n  summarize(total_words = sum(str_count(text, \"\\\\S+\")))\n\n# A tibble: 3 × 2\n  author   total_words\n  &lt;chr&gt;          &lt;int&gt;\n1 Hamilton      114076\n2 Jay             8513\n3 Madison        69377\n\n\nUnsurprisingly, Hamilton wrote like he was running out of time and ended with 114,076 words across his many authored papers."
  },
  {
    "objectID": "Riskprofile.html",
    "href": "Riskprofile.html",
    "title": "English Education in Small Towns",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 4)\n\n---- Compiling #TidyTuesday Information for 2024-01-23 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"english_education.csv\"\n\nenglish_education &lt;- tuesdata$english_education\n\nIn this analysis, I will utilize the data from TidyTuesday 2024-01-23 to perform a permutation test. The goal of the test is to confirm the claims of the article in which the data was used to answer the question: Is there a significant difference in education scores between different-sized towns? The data is taken from the UK Office for National Statistics. At a glance at the data, there was a significantly higher number of data points for small towns. The second closest recorded number was that of medium towns so they were chosen as the closest fair comparison.\nLink to the data:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-01-23/readme.md\n\nsize_interest &lt;- english_education |&gt;\n  filter(size_flag == \"Medium Towns\"| size_flag ==\"Small Towns\")\nhead(size_interest)\n\n# A tibble: 6 × 31\n  town11cd  town11nm  population_2011 size_flag rgn11nm coastal coastal_detailed\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;           \n1 E34000007 Carlton …            5456 Small To… East M… Non-co… Smaller non-coa…\n2 E34000016 Dorchest…           19060 Small To… South … Non-co… Smaller non-coa…\n3 E34000020 Ely BUA             19090 Small To… East o… Non-co… Smaller non-coa…\n4 E34000026 Market W…            6429 Small To… Yorksh… Non-co… Smaller non-coa…\n5 E34000027 Downham …           10884 Small To… East o… Non-co… Smaller non-coa…\n6 E34000039 Penrith …           15181 Small To… North … Non-co… Smaller non-coa…\n# ℹ 24 more variables: ttwa11cd &lt;chr&gt;, ttwa11nm &lt;chr&gt;,\n#   ttwa_classification &lt;chr&gt;, job_density_flag &lt;chr&gt;, income_flag &lt;chr&gt;,\n#   university_flag &lt;chr&gt;, level4qual_residents35_64_2011 &lt;chr&gt;,\n#   ks4_2012_2013_counts &lt;dbl&gt;,\n#   key_stage_2_attainment_school_year_2007_to_2008 &lt;dbl&gt;,\n#   key_stage_4_attainment_school_year_2012_to_2013 &lt;dbl&gt;,\n#   level_2_at_age_18 &lt;dbl&gt;, level_3_at_age_18 &lt;dbl&gt;, …\n\n\n\npermutation_test &lt;- function(reps,data) {\n  data |&gt;\n    select(size_flag, education_score)|&gt;\n    mutate(edu_perm = sample(education_score, replace = FALSE)) |&gt;\n    group_by(size_flag) |&gt;\n    summarize(mean_score = mean(education_score, na.rm = TRUE), mean_perm = mean(edu_perm)) |&gt;\n    summarize(mean_diff = abs(diff(mean_score)), mean_perm_diff = diff(mean_perm))\n  }\n\nset.seed(123)\nunder_null &lt;- map(1:10000, permutation_test, data = size_interest) |&gt;\n  list_rbind()\n\n\nggplot(under_null, aes(x = mean_perm_diff)) +\n  geom_histogram(binwidth = 0.1, color = \"black\", alpha = 0.7) +\n  geom_vline(aes(xintercept = mean_diff, color = \"red\")) +\n  labs(title = \"Distribution of Permutation Under Null\",\n       x = \"Difference in Mean Scores\",\n       y = \"Frequency\") \n\n\n\n\n\n\n\n\nFrom the plot it can be seen that the line representing the observational measurement of education score differences is off-centered. There is a small number of occurrences of simulated under the null. This would mean that the observed difference is more extreme than data created randomly.\n\nunder_null|&gt;\nsummarize(p_value = mean(mean_perm_diff &gt; mean_diff))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0139\n\n\nAfter creating 10000 permutation simulations, the distribution under the null hypothesis falls on zero as there is no expected difference between the two groups. From the results of the p-value, if the significance level, as it usually is, is set to be 0.05, our p-value fall below this level. \\(\\alpha\\) &gt; 0.0139. This means that our data is significantly extreme and would probably not occur by chance."
  }
]