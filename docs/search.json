[
  {
    "objectID": "sql.html",
    "href": "sql.html",
    "title": "SQL",
    "section": "",
    "text": "Better drivers - Comparison of proportion of drivers issues a citation by the state patrol in CA vs FL\n\ndbListTables(con_traffic)\n\n [1] \"ar_little_rock_2020_04_01\"      \"az_gilbert_2020_04_01\"         \n [3] \"az_mesa_2023_01_26\"             \"az_statewide_2020_04_01\"       \n [5] \"ca_anaheim_2020_04_01\"          \"ca_bakersfield_2020_04_01\"     \n [7] \"ca_long_beach_2020_04_01\"       \"ca_los_angeles_2020_04_01\"     \n [9] \"ca_oakland_2020_04_01\"          \"ca_san_bernardino_2020_04_01\"  \n[11] \"ca_san_diego_2020_04_01\"        \"ca_san_francisco_2020_04_01\"   \n[13] \"ca_san_jose_2020_04_01\"         \"ca_santa_ana_2020_04_01\"       \n[15] \"ca_statewide_2023_01_26\"        \"ca_stockton_2020_04_01\"        \n[17] \"co_aurora_2023_01_26\"           \"co_denver_2020_04_01\"          \n[19] \"co_statewide_2020_04_01\"        \"ct_hartford_2020_04_01\"        \n[21] \"ct_statewide_2020_04_01\"        \"fl_saint_petersburg_2020_04_01\"\n[23] \"fl_statewide_2020_04_01\"        \"fl_tampa_2020_04_01\"           \n[25] \"ga_statewide_2020_04_01\"        \"ia_statewide_2020_04_01\"       \n[27] \"id_idaho_falls_2020_04_01\"      \"il_chicago_2023_01_26\"         \n[29] \"il_statewide_2020_04_01\"        \"in_fort_wayne_2020_04_01\"      \n[31] \"ks_wichita_2023_01_26\"          \"ky_louisville_2023_01_26\"      \n[33] \"ky_owensboro_2020_04_01\"        \"la_new_orleans_2020_04_01\"     \n[35] \"ma_statewide_2020_04_01\"        \"md_baltimore_2020_04_01\"       \n[37] \"md_statewide_2020_04_01\"        \"mi_statewide_2020_04_01\"       \n[39] \"mn_saint_paul_2020_04_01\"       \"mo_statewide_2020_04_01\"       \n[41] \"ms_statewide_2020_04_01\"        \"mt_statewide_2023_01_26\"       \n[43] \"nc_charlotte_2020_04_01\"        \"nc_durham_2020_04_01\"          \n[45] \"nc_fayetteville_2020_04_01\"     \"nc_greensboro_2020_04_01\"      \n[47] \"nc_raleigh_2020_04_01\"          \"nc_statewide_2020_04_01\"       \n[49] \"nc_winston_salem_2020_04_01\"    \"nd_grand_forks_2020_04_01\"     \n[51] \"nd_statewide_2020_04_01\"        \"ne_statewide_2020_04_01\"       \n[53] \"nh_statewide_2020_04_01\"        \"nj_camden_2020_04_01\"          \n[55] \"nj_statewide_2020_04_01\"        \"nv_henderson_2020_04_01\"       \n[57] \"nv_statewide_2020_04_01\"        \"ny_albany_2020_04_01\"          \n[59] \"ny_statewide_2020_04_01\"        \"oh_cincinnati_2020_04_01\"      \n[61] \"oh_columbus_2020_04_01\"         \"oh_statewide_2020_04_01\"       \n[63] \"ok_oklahoma_city_2023_01_26\"    \"ok_tulsa_2020_04_01\"           \n[65] \"or_statewide_2020_04_01\"        \"pa_philadelphia_2020_04_01\"    \n[67] \"ri_statewide_2020_04_01\"        \"sc_statewide_2020_04_01\"       \n[69] \"sd_statewide_2020_04_01\"        \"tn_nashville_2020_04_01\"       \n[71] \"tn_statewide_2020_04_01\"        \"tx_arlington_2020_04_01\"       \n[73] \"tx_austin_2020_04_01\"           \"tx_garland_2020_04_01\"         \n[75] \"tx_houston_2023_01_26\"          \"tx_lubbock_2020_04_01\"         \n[77] \"tx_plano_2020_04_01\"            \"tx_san_antonio_2023_01_26\"     \n[79] \"tx_statewide_2020_04_01\"        \"va_statewide_2020_04_01\"       \n[81] \"vt_burlington_2023_01_26\"       \"vt_statewide_2020_04_01\"       \n[83] \"wa_seattle_2020_04_01\"          \"wa_statewide_2020_04_01\"       \n[85] \"wa_tacoma_2020_04_01\"           \"wi_madison_2023_01_26\"         \n[87] \"wi_statewide_2020_04_01\"        \"wy_statewide_2020_04_01\"       \n\n\n\nSELECT\n    state,\n    COUNT(*) AS total_stops,\n    SUM(citations) AS citations,\n    ROUND(SUM(citations) / COUNT(*) * 100, 2) AS citation_rate_percent\nFROM (\n    SELECT\n        'CA' AS state,\n        s.citation_issued,\n        CASE WHEN s.citation_issued = TRUE THEN 1 ELSE 0 END AS citations\n    FROM ca_statewide_2023_01_26 s\n    WHERE s.citation_issued IS NOT NULL\n\n    UNION ALL\n\n    SELECT\n        'FL' AS state,\n        s.citation_issued,\n        CASE WHEN s.citation_issued = TRUE THEN 1 ELSE 0 END AS citations\n    FROM fl_statewide_2020_04_01 s\n    WHERE s.citation_issued IS NOT NULL\n) combined\nGROUP BY state\nORDER BY citation_rate_percent DESC;\n\n\nSELECT\n    COUNT(*) AS total_searches,\n    SUM(CASE WHEN s.contraband_found = TRUE THEN 1 ELSE 0 END) AS searches_with_contraband,\n    ROUND(SUM(CASE WHEN s.contraband_found = TRUE THEN 1 ELSE 0 END) / COUNT(*) * 100, 2) AS contraband_hit_rate_percent\nFROM\n    pa_philadelphia_2020_04_01 s\nWHERE\n    s.search_conducted = TRUE\n    AND s.contraband_found IS NOT NULL;\n\n\ncontr_table\n\n  total_searches searches_with_contraband contraband_hit_rate_percent\n1         116455                    33230                       28.53\n\n\n\nggplot(contr_table, aes(y = total_searches, fill = searches_with_contraband)) +\n  geom_bar( width = 0.6) +\n  labs(\n    title = \"Searches vs. Contraband Found — Philadelphia, PA\",\n    x = \"\",\n    y = \"Number of Stops\"\n  )\n\nDon't know how to automatically pick scale for object of type &lt;integer64&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n\n\nggplot(prop_table, aes(x = state, y = citations)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"Proportion of Drivers Issued Citations by State Patrol\",\n    x = \"State\",\n    y = \"Citation Rate\"\n  ) +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\ndbDisconnect(con_traffic)\n\nData Source: Stanford Open Policing Project. Pierson, E., Corbett-Davies, S., & Goel, S. (2020). “A large-scale analysis of racial disparities in police stops across the United States.” Nature Human Behaviour, 4(7), 736–745. https://openpolicing.stanford.edu/"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Ethics in Predictive Analytics for Marketing",
    "section": "",
    "text": "Two articles:\n\nHow Target leveraged predictive analytics to improve marketing ROI. OvationMR. Retrieved from https://www.ovationmr.com/how-target-leveraged-predictive-analytics/\n\n\n\nDuhigg, C. (2012, February 16). How companies learn your secrets. The New York Times Magazine. https://www.nytimes.com/2012/02/19/magazine/shopping-habits.html\n\nSummary\nIn 2012, the New York Times released an article with the headline, “How Companies Learn Your Secrets”. The big story of this article was how Target asked a statistician, Andrew Pole, whom they hired to identify pregnant customers so they could tailor their marketing before any other company could capitalize on the spending habits of new parents. The article further dives into the work of predictive analytics and how companies use customer habits to determine how to better sell a product. The reporter, Charles Duhigg, states that when Target was made aware of his article, they stated that they were in compliance with Privacy Laws and critiqued inaccuracies in his reporting, but would not elaborate or meet with him for further discussion. Further development by Pole on the targeted advertisement was to address possibly scaring their customers by alerting them they knew their personal profile or information before they had a purchase history of it. To overcome this, a layer of mixed advertisement with targeted advertisement would hide this.\nOvationMR takes the prior article discussed as a launching point for their own advertisement. The website uses the New York Times article as a case study to learn strategies and marketing growth from. Calling its resolution a success, with Target being able to “maintain its customer relationships while respecting their privacy.”\n\nWhat was the consent structure for recruiting participants? Were the participants aware of the ways their data would be used for research? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen?\nFrom the New York Times article, it is shown the consent structure is not fully seen by the customer as it is a silent agreement that purchase history will be retained by the company. In this case, like in many marketing departments, a user profile is created for the customers to track engagement across what the store offers, including coupons, surveys, refunds, purchases, etc. The customers were not aware that Target would transform the data collected on them into a prediction system of their potential pregnancy status. Informed consent of the full situation was not sought after since this fits within company privacy laws that allow it to maintain information for marketing purposes, but a concent is implied to give the company data when purchasing from them. Although the purpose could be deduced early on when advertisements specifically showed baby items alone.\nWhat was the data collection process? Were the observations collected ethically? Are there missing observations?\nThe data collection process was based on identifying customer behavior patterns and technically on legal grounds and respecting privacy, as Target only made conclusions based on what customers would purchase. OvationMR encourages the practice of data tracking with existing models and third party resources that follow privacy laws. This further drives the point of widespread data tracking.\nIs the data identifiable? All of it? Some of it? In what way? Are the data sufficiently anonymized or old to be free of ethical concerns? Is anonymity guaranteed?\nWhile this data is not publicly available, in earlier advertising efforts, it could be deduced that pregnancy was implied and therefore gave a piece of private information to whoever encountered the advertisement in connection with the customer, as was the case for the angry father and his teenage daughter. The revised advertisement model better hides the privacy issue as it is theoretically undetectable even by the target customer. However, holding the information of someone being pregnant without their knowledge is a privacy issue on its own, as the customer has not sought to make this personal information available to Target. Forming such a sensible data set can risk customer privacy if there is a data breach. If third-party advertisement, as brought up by OvationMR, is taken into consideration, then information from Target can end up elsewhere.\n10.Respect and invite fair criticism while promoting the identification and open discussion of errors, risks, and unintended consequences of our work.\nOvationMR reviewed the criticism Target received as a need to reanalyze their marketing strategy rather than an overstep. From the New York Times, it is reported that Target refused to give a statement to Pole, which means in light of their predictive analytics, they failed a point of the manifesto. By refusing to engage in conversation over their data collection and analysis, they in some form acknowledged that it was not the most well-handled data. It refused direct criticism but answered to customer criticism not by removing the ads but by disguising them."
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Simpsons",
    "section": "",
    "text": "Introduction\nIn this project, I will use data from the TidyTuesday folder for February 2, 2025. The original data is from https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset, which is a kraggle data set authored by Prashant Banerjee. This data is further scraped by other users. The data is split into four data frames of characters, episodes, locations, and script lines. I focused on script line count among the characters with the over 2000 cumulative word counts.\nGraph and Code\n\nknown_character &lt;- simpsons_script_lines |&gt;\n  group_by(character_id) |&gt;\n  summarize(total_word_count = sum(word_count, na.rm = TRUE)) |&gt;\n  filter(total_word_count &gt; 2000)\nknown_character\n\n# A tibble: 16 × 2\n   character_id total_word_count\n          &lt;dbl&gt;            &lt;dbl&gt;\n 1            1            23903\n 2            2            52649\n 3            3             5023\n 4            8            22528\n 5            9            21675\n 6           11             4319\n 7           15             6841\n 8           17             8773\n 9           25             4075\n10           31             4815\n11           71             3962\n12          101             2067\n13          139             3857\n14          165             2944\n15          211             2271\n16         1078             2533\n\nknown_character|&gt;\n  ggplot(aes(x = character_id, y = total_word_count)) +\n  geom_point() +\n  labs(\n    title = \"Characters with the Highest Word Count\",\n    x = \"Character Identification\",\n    y = \"Total Word Count\"\n  )\n\n\n\n\n\n\n\n\nAnalysis\nFor this data, we can see that the characters that have said over 2000 words is limited to 16 characters. Characters that appear earlier in the character id line seem to be prominent to appear as prolific characters. The next step in this data is to identify a match between character id to character name across data frames to better understand which characters are favored for lines. If we were to exclude the main Simpson family, which characters would receive the most attention?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dataviz2.html",
    "href": "dataviz2.html",
    "title": "Groundhog",
    "section": "",
    "text": "Introduction\nThis project draws its data from the GitHub repository TidyTuesday’s 2024-01-30 folder titled “Groundhog Predictions”. This data is originally from https://groundhog-day.com/. This data puts together the predictions of groundhogs for 6 more weeks of winter or the coming of spring across the US and Canada.\n\nshadow_prediction  &lt;- predictions |&gt;\n  group_by(year) |&gt;\n  summarize(total_shadow_count = sum(shadow, na.rm = TRUE))\nshadow_prediction\n\n# A tibble: 138 × 2\n    year total_shadow_count\n   &lt;dbl&gt;              &lt;int&gt;\n 1  1886                  0\n 2  1887                  1\n 3  1888                  1\n 4  1889                  0\n 5  1890                  0\n 6  1891                  0\n 7  1892                  0\n 8  1893                  0\n 9  1894                  0\n10  1895                  0\n# ℹ 128 more rows\n\nggplot(shadow_prediction, aes(x = year, y = total_shadow_count)) +\n  geom_col()\n\n\n\n\n\n\n\n\nGraphing and Code\n\nid_prediction  &lt;- predictions |&gt;\n  group_by(id) |&gt;\n  summarize(total_shadow_count = sum(shadow, na.rm = TRUE))\n\nggplot(id_prediction,  aes(x = id, y = total_shadow_count)) +\n  geom_col() +\n  labs(\n    title = \"Longer winter predictions per Ground Hog\",\n    x = \"Groundhog identification\",\n    y = \"Saw Shadow Count\"\n  )\n\n\n\n\n\n\n\nggplot(predictions, aes(x = year, fill = shadow)) +\n  geom_bar() +\n  labs(\n    title = \"Yearly Shadow Predictions\",\n    x = \"Year\",\n    y = \"Total Predictions\"\n  )\n\n\n\n\n\n\n\n\nAnalysis\nThe first graph isolates the total count of observed shadows by year. Similarly, the third graph reiterates this idea by presenting the count of all shadow predictions per year. From these graphs, it is obvious that there is a growing number of predictions by different groundhogs. The growth in groundhog day popularity is especially visible when we view the third graph in conjunction with the second graph showing the consistency and recency in groundhog count of predictions. In the second graph, we can see that earlier groundhogs have had a greater impact on the data for winter predictions."
  },
  {
    "objectID": "FedPapers.html",
    "href": "FedPapers.html",
    "title": "Federalist Papers Analysis",
    "section": "",
    "text": "Introduction\nThis is project 2 which seeks to work with string and character data. The data used comes from nicholasjhorton’s GitHub repository which provides a data frame for the Federalist papers.\nEstablishing a Government\nSince the purpose of the Federalist Papers is to defend the Constitution that establishes the U.S government branches, I proceeded to examine how many times this purpose is linked through mentions of the words “government”. I provided an alternative “confederacy” due to its predecessor, the Articles of Confederation, as a name for government in reference to the United States.\n\nCountry_FedPapers &lt;- FederalistPapers |&gt;\n  mutate(Country_FedPapers = str_to_lower(text))|&gt;\n  group_by(paper) |&gt;\n  summarise(Word_count = sum(str_count(text,\"government\") + \n              str_count(text,\"confederacy\"))) \n\nAlthough counting the occurrence of the two words does not give any context to what direction the author intends to take the paper, the repetition of the words provides a hint.\nGraphing Count of “Government” and “Confederacy”\n\nggplot(Country_FedPapers, aes(x = paper, y = Word_count)) +\n  geom_col() + \n  labs(\n    x = \"Federalist Paper Number\",\n    y = \"Word Count\", \n    title = \"Number of times 'confederacy' or 'government' mentioned\"\n  )\n\n\n\n\n\n\n\n\nFrom the graph, we can see a consistent call upon government and confederacy with the highest count being around paper 45 which describes the power of the government relative to state governments, so it is possible the constant mention of the two forms of government caused the spike.\nTitle Length per Paper\nAnother alteration to the data I will apply is separating the title into its generic FEDERALIST No. and its assigned title. From here I analyze the length of each title.\n\nNeater_Fedpaper &lt;- FederalistPapers |&gt;\n  mutate(solo_title = str_extract(title,\"(?&lt;=\\\\. ).+\")) |&gt;\n  group_by(paper) |&gt;\n  summarize(title_length = str_length(first(solo_title)))\n\nGraph of Title Length vs Specific Word Count\nThen will the length of the title tell anything about the wordiness of the author? In other words, will the length of the title predict the amount of times the words confederacy and government appear?\n\nJoined_Fedpaper &lt;- left_join(Country_FedPapers, Neater_Fedpaper)\n\nJoining with `by = join_by(paper)`\n\nggplot(Joined_Fedpaper, aes( x = title_length , y = Word_count)) +\n  geom_point() +\n  labs(\n    x = \"Length of Title\",\n    y = \"Count of government and confederacy\",\n    title = \"Does title length predict utterances of government?\"\n  )\n\n\n\n\n\n\n\n\nThere is no easily visible marked trend. A longer title does not relate to a longer string of confederacies and government mentions that may appear in longer papers.\nConstitution and What Else?\nIn an attempt to contextualize what is referred to as the Constitution in the Federalist papers, the following code draws out the words following the word constitution.\n\nFederalistPapers |&gt;\n  filter(str_detect(text, \"(?i)constitution\"))|&gt;\n  group_by(paper)|&gt;\n  summarize(surr_const = str_c(unlist(\n    str_extract_all(text, \"(?i)constitution\\\\s+(\\\\w+)\")), collapse = \", \"))  \n\n# A tibble: 77 × 2\n   paper surr_const                                                            \n   &lt;dbl&gt; &lt;chr&gt;                                                                 \n 1     1 \"Constitution for, CONSTITUTION TO, CONSTITUTION and, Constitution or\"\n 2     6 \"\"                                                                    \n 3     7 \"\"                                                                    \n 4     8 \"constitution that\"                                                   \n 5     9 \"constitution that\"                                                   \n 6    10 \"Constitution forms\"                                                  \n 7    14 \"Constitution From, Constitution are, Constitution is\"                \n 8    15 \"constitution of\"                                                     \n 9    16 \"Constitution which, constitution in\"                                 \n10    17 \"\"                                                                    \n# ℹ 67 more rows\n\n\nAlthought the code does not give much context as the words following the mention of constitution are largely filler words, it does highlight the different instances of constitution. The capitalized Constitution is likely a direct reference to the U.S. Constitution. The completely capitalized CONSTITUTION is likely referenced in an address to the reader or some outstanding instance while the completely lowercase constitution is the general constitute of something.\nWho Wrote the Most?\nFrom a brief look at the authorship of each paper, it can be seen that Hamilton took authorship of most, but how many words did they write across their papers?\n\n FederalistPapers |&gt;\n  group_by(author) |&gt;\n  summarize(total_words = sum(str_count(text, \"\\\\S+\")))\n\n# A tibble: 3 × 2\n  author   total_words\n  &lt;chr&gt;          &lt;int&gt;\n1 Hamilton      114076\n2 Jay             8513\n3 Madison        69377\n\n\nUnsurprisingly, Hamilton wrote like he was running out of time and ended with 114,076 words across his many authored papers."
  },
  {
    "objectID": "Riskprofile.html",
    "href": "Riskprofile.html",
    "title": "English Education in Small Towns",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 4)\n\n---- Compiling #TidyTuesday Information for 2024-01-23 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"english_education.csv\"\n\nenglish_education &lt;- tuesdata$english_education\n\nIn this analysis, I will utilize the data from TidyTuesday 2024-01-23 to perform a permutation test. The goal of the test is to confirm the claims of the article in which the data was used to answer the question: Is there a significant difference in education scores between different-sized towns? The data is taken from the UK Office for National Statistics. At a glance at the data, there was a significantly higher number of data points for small towns. The second closest recorded number was that of medium towns so they were chosen as the closest fair comparison.\nLink to the data:\nhttps://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-01-23/readme.md\n\nsize_interest &lt;- english_education |&gt;\n  filter(size_flag == \"Medium Towns\"| size_flag ==\"Small Towns\")\nhead(size_interest)\n\n# A tibble: 6 × 31\n  town11cd  town11nm  population_2011 size_flag rgn11nm coastal coastal_detailed\n  &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;           \n1 E34000007 Carlton …            5456 Small To… East M… Non-co… Smaller non-coa…\n2 E34000016 Dorchest…           19060 Small To… South … Non-co… Smaller non-coa…\n3 E34000020 Ely BUA             19090 Small To… East o… Non-co… Smaller non-coa…\n4 E34000026 Market W…            6429 Small To… Yorksh… Non-co… Smaller non-coa…\n5 E34000027 Downham …           10884 Small To… East o… Non-co… Smaller non-coa…\n6 E34000039 Penrith …           15181 Small To… North … Non-co… Smaller non-coa…\n# ℹ 24 more variables: ttwa11cd &lt;chr&gt;, ttwa11nm &lt;chr&gt;,\n#   ttwa_classification &lt;chr&gt;, job_density_flag &lt;chr&gt;, income_flag &lt;chr&gt;,\n#   university_flag &lt;chr&gt;, level4qual_residents35_64_2011 &lt;chr&gt;,\n#   ks4_2012_2013_counts &lt;dbl&gt;,\n#   key_stage_2_attainment_school_year_2007_to_2008 &lt;dbl&gt;,\n#   key_stage_4_attainment_school_year_2012_to_2013 &lt;dbl&gt;,\n#   level_2_at_age_18 &lt;dbl&gt;, level_3_at_age_18 &lt;dbl&gt;, …\n\n\n\npermutation_test &lt;- function(reps,data) {\n  data |&gt;\n    select(size_flag, education_score)|&gt;\n    mutate(edu_perm = sample(education_score, replace = FALSE)) |&gt;\n    group_by(size_flag) |&gt;\n    summarize(mean_score = mean(education_score, na.rm = TRUE), mean_perm = mean(edu_perm)) |&gt;\n    summarize(mean_diff = abs(diff(mean_score)), mean_perm_diff = diff(mean_perm))\n  }\n\nset.seed(123)\nunder_null &lt;- map(1:10000, permutation_test, data = size_interest) |&gt;\n  list_rbind()\n\n\nggplot(under_null, aes(x = mean_perm_diff)) +\n  geom_histogram(binwidth = 0.1, color = \"black\", alpha = 0.7) +\n  geom_vline(aes(xintercept = mean_diff, color = \"red\")) +\n  labs(title = \"Distribution of Permutation Under Null\",\n       x = \"Difference in Mean Scores\",\n       y = \"Frequency\") \n\n\n\n\n\n\n\n\nFrom the plot it can be seen that the line representing the observational measurement of education score differences is off-centered. There is a small number of occurrences of simulated under the null. This would mean that the observed difference is more extreme than data created randomly.\n\nunder_null|&gt;\nsummarize(p_value = mean(mean_perm_diff &gt; mean_diff))\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0139\n\n\nAfter creating 10000 permutation simulations, the distribution under the null hypothesis falls on zero as there is no expected difference between the two groups. From the results of the p-value, if the significance level, as it usually is, is set to be 0.05, our p-value fall below this level. \\(\\alpha\\) &gt; 0.0139. This means that our data is significantly extreme and would probably not occur by chance."
  }
]